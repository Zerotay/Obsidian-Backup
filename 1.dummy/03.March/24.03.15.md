# 캐싱
고품질을 위해. 
성능, 안정성, 보안
성능이 높아질수록 안정성은 떨어진다.

- 브라우저 단위의 캐시
	- 이미지 등의 정적 리소스 대상
	- 서버의 리소스가 실시간으로 반영되진 않음
- 응답 지연 값
	- 일반적인 io에서... 읽는 속도는 메모리가 압도적으로 빠름

# 엘라스틱 서치
![[Pasted image 20240315121401.png]]
그제 있었던 이슈. 컨테이너를 세워도 제대로 유지되지 않았다. 
궁금해서 log를 꺼내보니 다음과 같은 메시지를 볼 수 있었다.
root 계정으로 하지 말라는 것. 
![[Pasted image 20240315121449.png]]
그래서 이렇게 바꿔줬더니 일단 잘 돌아간다!

다음 이슈는 내가 설치한 버전. 나는 가장 최신 버전인 8.12.2
7.17.3으로 버전을 맞춰야 스프링 부트 버전과 배치 버전과 호환되게 만들 수 있다. 
현재 팀에서 사용하는 스프링부트는 2.7x이다. 3으로 넘어갈 경우에는 문제 없이 엘서를 8로 사용할 수 있을 것으로 생각되나.
![[Pasted image 20240315122846.png]]
성공. 8버전은 무조건 ssl로 연결해야 한다는 번거로움까지 있다. 그러나 7의 경우는 그럴 필요가 없어서 최소한 연결하는 입장에서는 조금 더 편리하다.
물론 그렇다해도 ssl 터미네이션시키면 돼서 걱정은 없긴 하다. 

이 친구도 디비 역할을 하기에 논리 볼륨을 만들어줄 필요가 있다.






# 클라우드 강의

kubelet이 노드 속 레지스트리..?
도커 빌드는 캐싱이 되던데, copy 같은 명령어의 수정사항은 어떻게 반영이 되는가? 알아서 파일의 변경 사항을 어떻게 알아채는가?

오늘은 디플로이먼트 객체와 서비스 객체 생성할 것이다.
쿠버에 필요한 요소는 미리 선언만 하면 된다. yaml. 이걸 보통 manifest라고 한다.
가능하면 쿠버는 선언적으로 동작시키는 게 좋다.

디플로이먼트는 그 아래 레플리카셋. 그 아래로 3개의 파드가 있다.

파드들은 각자 같은 일한다. 그러니 이들을 같은 동작을 처리하기에 이를 해결하는 네트워크 필요.
로드밸런싱!

파드는 각자가 하나의 호스트라 생각하면 쉽다. 각자 고유한 ip를 할당받는다.
파드가 할당될 ip를 줄 풀이 필요하다. 

디플 객체를 만들 때 kind에 deployment라 정의한다.
안에 spec에는 replica는 3개를 하도록
그 안 템플릿에 또 비슷한 형시긍로
거기에는 labels를 지정한다.
template안 spec에 제대로 컨테이너를 지정

레플리카셋에 정의된 내용으로 알아서 늘렸따 줄였따 한다. 이걸 hpa라고 한다. 
최소값, 최대값
replicas는 replicaset의 복제 수이다.
replicaset은 늘었따 줄었따 한다. 몇개의 복제본을 가질거냐 이런 얘기

gui로도 가능하다. 
gui를 먼저 실습하자.
개념 설명하고 겹쳐서

gui로 만들 때 yml 확인 가능
이때 hpa가 있따. horizontalpopautuscaler
이걸로 레플리카셋을 늘리다 말았따 한다.
hpa도 하나의 객체이다. 

서비스를 할 때는?
프런트엔드 파드를 두고,
중간에 서비스를 둔 다음 백엔드 파드들에 전달한다. 
서비스란 건 쿠버 안에서 동작하는 로드밸런서처럼 생각하면 된다.
파드 집합을 네트워크 상에서 안정적으로 추상화하는 객체이다.

사용자 흐름 생각해보자.
사용자가 클라 거쳐서 한 백엔드 파드로가보자.
그리고 로그인하면?
한 파드에는 세션에 남는데..
사용하다가 다른 파드로 가게 된다면!
문제 생긴다.
그래서 세션에 두지 말고 레디스나 memcached같은 달느 저장소에 둬야 한다. 

파드 단위의 네트워크가 있따. eth0.
그 안에도 네트워크가 있따. 공유 네트워킹 네임스페이스
그 안에서는 로컬호스트가 동작함

한 노드 안에서 파드가 두개일 때...
각 파드는 ip할당받는다.
노드의 넷인페로부터
한 파드가 받게 되는, 그 안에서 돌아가는 네트워크. 그걸 veth라 한다. 
노드의 nic와 veth 사이에 안 보이는 스위치가 있을 것이다.
이들은 노드 네트워킹 네임스페이스 상에서 통신할 것이다. 
이걸 브릿지 네트워크라고 부른다.

파드에는 두가지 넷이 있는 꼴이다.
파드 내에서, 그리고 노드 내에서.
더 위로 가면 클러스터 내에서의 넷도 있다..

강사 왈 컨테 간 통신은 브릿지가 아니라한다. 어차피 이쪽 안은 로컬호스트로 통신한다고.
도커 네트워크와는 조금 다른가보다.

노드 간 통신을 해야할 때. 어떻게 하지? 노드는 서로의 ip를 모르자나. 
이때 까는 게 바로 cni이다.
이걸 쓰면 한 vpc안에 모든 파드가 동일 선상에 있는 것처럼 사용할 수 있게된다. 
하나의 호스트인 것처럼.
그래서 그 안에 파드들은 전부 이름이고유해야 하는 것이다. 

파드 내 컨테간 통신. 가능
한 노드 안에 파드 복수. 이들도 통신 가능. 뭔가 스위치가 있는 마냥 연결된다.이게 브릿지
여기에서 한번더 확장
노드 복수. 이들의 통신. 
vpc에 바로 파드들이 붙어있는 거마냥 해주는 게 cni

서브넷 세부 정보를 보면
보조 ip범위가 있따.
하나는 파드, 하나는 서비스 용
파드가 많으니 당연히 호스트 범위도 넓다. cidr이 14로
서비스는 파드들에 대한 로드밸런서. 네트워크 추상화 객체. 부하분산 가능케해줌
엄밀하게 서비스도 하나의 파드. 로드밸런싱 역할을 할 뿐

저번 내용 생각하다가 궁금한 게 생겨서 질문 드립니다. 도커파일 빌드를 하면 레이어별로 캐싱이 되는 것 같던데, copy를 하는 어떤 파일이 변경된 것은 알아서 변경을 감지하고 그 부분을 다시 빌드를 하는 것 같았습니다. 도커에서는 어떻게 파일이 바뀐 것을 감지하고 다시 빌드를 하는 것인지 궁금합니다. 
도커파일 상에서 변경된 코드는 없는데도 copy를 하는 파일이 조금 수정되도 그게 다시 빌드되더라구요

클러스터 ip 서비스에 고정 ip주소를 포함한다.
프런트엔드는 서비스 ip만 알면 로드밸런싱이 된다.
따로 지정 안할시 ClusterIP가 디폴트이다.
이건 클러스터 내부에서만 접속된다.

서비스가 kind
안속 spec에 type은 clusterip
안속 selector가 매우 중요하다.
app: backend, 아니면 frontend 돼있다.
이 app이 서비스의 핵심 같은 게 된다.
backend:3306으로 들어가면 6000으로 드가는 꼴

타입이 NodePort여야 하는 경우도 있따.
노드마다 지정된 포트를 연다. 클라는 해당 노드의 ip를 안다면 될 것이다.
근데 클라가 일일히 ip, 포트 정보 아는 것도 어렵다.
그래서 로드밸런서라라는 타입도 사용한다.
서비스 스펙에 clusterip, nodeport, loadbalancer타입이 있는 건가요
맞다

clusterip는 클러스터내부에서만
nodeport나 loadbalancer로 해야만 클러스터 외부에서도 접근이 가능해진다.
(이 둘의 정확한 차이가 뭐지)

블루그린을 배포할 때!
블루는 label에 app:v1이라 태깅될 것.
서비스는 이 앞에 서있따.
이때 셀렉터가 쓰인다.
서비스의 셀렉터만 바꿔치면 새로운 그린으로 트래픽이 간다.

카나리아 배포일 때는?
각 디플은  my-app이라 써잇을 것이다.
각 파드는 거기에서 version이 v1이고, v2일 것.
```
	label:
		app:my-app
		version:v1
		
	label:
		app:my-app
		version:v2
```
이때 그냥 my-app으로 해두면 양쪽으로 흘러간다.

디플 스펙을 정의할 때 label을 지정해야 한다.
서비스는 자신이 볼 파드를 찾아야만 한다.
이때 selector를 사용한다.
카나리는 실환경 테스트가 가능하단 게 가장 크다.

셀렉터가 참 중요하다.

필요하면 롤백도 가능하다.
rollout. revision을 해주면 된다.
사용다하면 디플 삭제도 가능. 

디플을 쓰면 쉽게 서버를 만들 수있따
근데 일일히 이름 알기는 어려우니..
써비스를 쓰짜

cloud functions.
이건 코드만 박으면 그냥 실행된다.
서버레스하다. 이벤트 드리븐이라고 한다.
aws의 lambda같은 놈이다.

cloud run이란 게 있따.
코드를 컨테이너화해서 다른 곳으로 넘기는 기능.
컨테이너만 배포할 수 있게

오토파일럿쓰면 노드는 자동관리. 컨테이너만 배포하면 된다.

서비스들의 서비스는 잉그레스라 부른다.
이벤트 드리븐이 무얼 뜼하는 건지 궁금합니다.
이게 뭐냐, 클라우드 비용이 많이 들잖냐. 변경이나 요청이 들어올 때만 동작하게 하는 게 효율적이다. 처리할 게 끝나면 자원이 폐기한다.